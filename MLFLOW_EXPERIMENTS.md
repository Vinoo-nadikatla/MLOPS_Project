# MLflow Experiments Report

## ğŸ“Š Experiments Overview

Your MLflow tracking has recorded **1 active experiment** with **1 completed run**.

---

## ğŸ”¬ Experiment 1: Health Experiment

**Experiment ID:** `606211225731357525`  
**Status:** Active âœ…  
**Created:** 2025-01-02 (timestamp: 1764734612228)

---

## ğŸ“ˆ Run Details

### Run: `bouncy-mole-883`

**Run ID:** `6df6f65520894bb886da66be62352ee0`  
**Status:** Completed âœ…  
**Duration:** 301 ms (12:34:12 UTC)  
**User:** braje

---

## ğŸ“Š Metrics Tracked

| Metric | Value | Description |
|--------|-------|-------------|
| **rf_val_acc** | **0.9854** (98.54%) | Random Forest validation accuracy |
| **optimal_assignment_score** | **102.12** | Hungarian algorithm assignment score |
| **n_samples** | **205** | Number of validation samples |

### Performance Interpretation

- **RF Validation Accuracy: 98.54%** âœ¨
  - Excellent classification performance
  - Very low error rate (~1.46%)
  - Model generalizes well to unseen data

- **Optimal Assignment Score: 102.12** ğŸ“
  - Hungarian algorithm found optimal assignments
  - Score represents total cost/benefit of assignments
  - Higher score may indicate good resource allocation

- **Validation Set Size: 205 samples** ğŸ“‹
  - Test set from train-test split (20% of 1025)
  - Good sample size for validation
  - Reduces overfitting risk

---

## âš™ï¸ Hyperparameters

| Parameter | Value | Description |
|-----------|-------|-------------|
| **n_estimators** | 100 | Number of trees in Random Forest |
| **test_size** | 0.2 | Train-test split ratio (80-20) |
| **random_state** | 42 | Random seed for reproducibility |

### Why These Values?

```
n_estimators = 100
â”œâ”€ Standard baseline for Random Forests
â”œâ”€ Good balance between accuracy and speed
â””â”€ Prevents overfitting with sufficient diversity

test_size = 0.2
â”œâ”€ Standard 80-20 train-test split
â”œâ”€ 205 validation samples (good size)
â””â”€ 820 training samples (sufficient for learning)

random_state = 42
â”œâ”€ Ensures reproducible results
â”œâ”€ Allows others to replicate experiments
â””â”€ Good for debugging and validation
```

---

## ğŸ—ï¸ Model Architecture

### Stage 1: Random Forest Classifier
- **Type:** Ensemble method
- **Trees:** 100
- **Features:** 13 medical input features
- **Output:** Binary classification (Risk/No Risk)
- **Performance:** 98.54% accuracy

### Stage 2: Hungarian Algorithm
- **Type:** Optimal assignment algorithm
- **Purpose:** Assign patients to hospital resources
- **Method:** Linear sum assignment (scipy)
- **Score:** 102.12

---

## ğŸ“ Artifacts & Artifacts Location

**Artifact Path:**  
```
file:///D:/Brajesh/GitHubDsktop/MLOPS_Project/mlruns/606211225731357525/6df6f65520894bb886da66be62352ee0/artifacts
```

**Available Artifacts:**
- Model files
- Preprocessing configuration
- Predictions
- Metrics logs

---

## ğŸ·ï¸ Tags

| Tag | Value |
|-----|-------|
| `mlflow.runName` | bouncy-mole-883 |
| `mlflow.source.type` | 4 (Script) |
| `mlflow.user` | braje |

---

## ğŸ“Š Data Flow

```
Heart Disease Dataset (1025 samples)
â”‚
â”œâ”€ Training Set (80% = 820 samples)
â”‚  â””â”€ Used to train Random Forest
â”‚
â””â”€ Validation Set (20% = 205 samples)
   â”œâ”€ RF Validation Accuracy: 98.54%
   â””â”€ Hungarian Assignment Score: 102.12
```

---

## ğŸ¯ Model Quality Assessment

### Strengths âœ…

| Aspect | Status |
|--------|--------|
| **Accuracy** | Excellent (98.54%) âœ¨ |
| **Validation Strategy** | Proper train-test split âœ… |
| **Reproducibility** | Fixed random seed âœ… |
| **Parameter Tracking** | All logged âœ… |
| **Metrics Captured** | Multiple metrics âœ… |

### Considerations ğŸ“‹

- Single experiment recorded
- Would benefit from hyperparameter tuning experiments
- Cross-validation could improve robustness
- Multiple runs for comparison not yet created

---

## ğŸ”„ How to Log New Experiments

To run new experiments and compare results:

```python
import mlflow
import mlflow.sklearn

# Set experiment
mlflow.set_experiment("health_experiment")

# Start run
with mlflow.start_run(run_name="experiment_v2"):
    # Log parameters
    mlflow.log_param("n_estimators", 200)
    mlflow.log_param("test_size", 0.2)
    
    # Train model
    # ... training code ...
    
    # Log metrics
    mlflow.log_metric("accuracy", 0.95)
    mlflow.log_metric("f1_score", 0.92)
    
    # Log model
    mlflow.sklearn.log_model(model, "model")
```

---

## ğŸ“ˆ Viewing in MLflow UI

To start MLflow UI and view experiments:

```powershell
# Navigate to project directory
cd D:\Brajesh\GitHubDsktop\MLOPS_Project

# Start MLflow UI
mlflow ui

# Open in browser: http://localhost:5000
```

**MLflow UI Features:**
- Visual comparison of runs
- Parameter and metric tracking
- Model artifact downloads
- Experiment comparison charts

---

## ğŸ“ Recommended Next Steps

### 1. Hyperparameter Tuning
```python
# Try different n_estimators
for n_trees in [50, 100, 150, 200]:
    # Train and log each experiment
    mlflow.log_param("n_estimators", n_trees)
```

### 2. Cross-Validation
```python
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
mlflow.log_metric("cv_mean_score", scores.mean())
```

### 3. Feature Importance Analysis
```python
# Log feature importances
importances = model.feature_importances_
for i, imp in enumerate(importances):
    mlflow.log_metric(f"feature_{i}_importance", imp)
```

### 4. Experiment Comparison
```python
# Create multiple runs with different configs
configs = [
    {"n_estimators": 50},
    {"n_estimators": 100},
    {"n_estimators": 200},
]
# Log each and compare in UI
```

---

## ğŸ” File Structure

```
mlruns/
â”œâ”€â”€ 0/                                    (Default Experiment)
â”‚   â””â”€â”€ meta.yaml
â”‚
â””â”€â”€ 606211225731357525/                   (health_experiment)
    â”œâ”€â”€ meta.yaml                          (Experiment metadata)
    â””â”€â”€ 6df6f65520894bb886da66be62352ee0/ (Run)
        â”œâ”€â”€ meta.yaml                      (Run metadata)
        â”œâ”€â”€ artifacts/                     (Model & files)
        â”œâ”€â”€ metrics/
        â”‚   â”œâ”€â”€ n_samples                  (205)
        â”‚   â”œâ”€â”€ optimal_assignment_score   (102.12)
        â”‚   â””â”€â”€ rf_val_acc                 (0.9854)
        â”œâ”€â”€ params/
        â”‚   â”œâ”€â”€ n_estimators               (100)
        â”‚   â”œâ”€â”€ random_state               (42)
        â”‚   â””â”€â”€ test_size                  (0.2)
        â””â”€â”€ tags/
            â”œâ”€â”€ mlflow.runName
            â”œâ”€â”€ mlflow.source.git.commit
            â”œâ”€â”€ mlflow.source.name
            â”œâ”€â”€ mlflow.source.type
            â””â”€â”€ mlflow.user
```

---

## ğŸ“Š Quick Statistics

```
Total Experiments:     1 active
Total Runs:            1 completed
Best Accuracy:         98.54%
Best Assignment Score: 102.12
Validation Samples:    205
Training Samples:      ~820
Total Dataset:         1025
```

---

## ğŸ¯ Summary

Your MLflow tracking setup is working correctly! 

**Key Achievements:**
- âœ… Two-stage model successfully trained
- âœ… High validation accuracy (98.54%)
- âœ… All metrics properly logged
- âœ… Hyperparameters tracked
- âœ… Reproducible setup (random_state = 42)

**Next Steps:**
1. View experiments in MLflow UI: `mlflow ui`
2. Create multiple runs with different hyperparameters
3. Use MLflow to compare and select best model
4. Deploy best model to production

---

**Generated:** December 3, 2025  
**Status:** All systems operational âœ…
