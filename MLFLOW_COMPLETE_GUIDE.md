# ğŸ¯ Complete MLflow Guide & Reference

## ğŸ“Š Your Experiments at a Glance

### Current Status
```
Experiments:           1 active
Runs:                  1 completed
Best Accuracy:         98.54% â­â­â­â­â­
Dashboard:             http://127.0.0.1:5000 âœ…
Status:                ğŸŸ¢ OPERATIONAL
```

---

## ğŸš€ Quick Start

### View Your Experiments

```powershell
# Terminal 1: Start MLflow UI
cd D:\Brajesh\GitHubDsktop\MLOPS_Project
mlflow ui

# Then open in browser: http://127.0.0.1:5000
```

### Key Experiment Details

| Category | Details |
|----------|---------|
| **Experiment Name** | health_experiment |
| **Experiment ID** | 606211225731357525 |
| **Current Run** | bouncy-mole-883 |
| **Run Status** | âœ… Completed |
| **Model Accuracy** | 98.54% |

---

## ğŸ“ˆ Performance Dashboard

### Accuracy Metrics

```
Random Forest Validation Accuracy
â”œâ”€ Score:      98.54% (0.9854)
â”œâ”€ Correct:    202 out of 205
â”œâ”€ Error Rate: 1.46% (3 incorrect)
â””â”€ Grade:      A+ EXCELLENT
```

### Assignment Metrics

```
Hungarian Algorithm (Stage 2)
â”œâ”€ Total Score:    102.12
â”œâ”€ Purpose:        Optimal patient-resource assignment
â”œâ”€ Method:         Linear sum assignment
â””â”€ Quality:        Optimal (mathematical guarantee)
```

### Dataset Metrics

```
Validation Set
â”œâ”€ Total Samples:  205
â”œâ”€ Percentage:     20% of full dataset (1,025)
â”œâ”€ Quality:        Good sample size
â””â”€ Representativeness: High
```

---

## âš™ï¸ Model Configuration

### Training Configuration

```python
# Random Forest Classifier
n_estimators = 100          # Number of trees
random_state = 42           # Random seed (reproducible)
test_size = 0.2             # Train-test split (80-20)
```

### Why These Values?

```
n_estimators = 100
  â””â”€ Standard for Random Forests
  â””â”€ Balance between accuracy and speed
  â””â”€ Prevents overfitting

test_size = 0.2
  â””â”€ Industry standard 80-20 split
  â””â”€ 205 test samples (adequate)
  â””â”€ 820 train samples (sufficient)

random_state = 42
  â””â”€ Ensures reproducible results
  â””â”€ Anyone can replicate your work
  â””â”€ Good for version control
```

---

## ğŸ¯ Experiment Comparison

### Single Run Performance

```
Experiment: health_experiment
Run Name: bouncy-mole-883
â”œâ”€ Accuracy: 98.54%
â”œâ”€ Precision: High (few false positives)
â”œâ”€ Error Rate: 1.46%
â”œâ”€ Sample Size: 205
â””â”€ Status: âœ… PASSING
```

### Benchmark Comparison

```
Typical ML Model Benchmarks:
â”œâ”€ Poor:           < 70%
â”œâ”€ Good:           70-85%
â”œâ”€ Excellent:      85-95%
â”œâ”€ Outstanding:    95-99%
â”œâ”€ Your Model:     98.54% â­â­â­â­â­ OUTSTANDING
```

---

## ğŸ“Š MLflow UI Features

### Accessing the Dashboard

1. **Start MLflow UI**
   ```powershell
   cd D:\Brajesh\GitHubDsktop\MLOPS_Project
   mlflow ui
   ```

2. **Open in Browser**
   - URL: http://127.0.0.1:5000
   - Wait for page to load
   - View your experiments

### Dashboard Tabs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  EXPERIMENTS  â”‚  RUNS  â”‚  MODELS  â”‚  CHARTS â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  â€¢ Experiment: health_experiment            â”‚
â”‚  â€¢ Run: bouncy-mole-883                     â”‚
â”‚  â€¢ Metrics: 3 tracked                       â”‚
â”‚  â€¢ Parameters: 3 logged                     â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### In the UI, You Can:

**Experiments Tab:**
- âœ… View all experiments
- âœ… Create new experiments
- âœ… Delete experiments
- âœ… Filter and search

**Runs Tab:**
- âœ… View all runs in experiment
- âœ… Compare metrics side-by-side
- âœ… Sort by performance
- âœ… Download artifacts
- âœ… View run details

**Models Tab:**
- âœ… Register models
- âœ… Track versions
- âœ… Compare model versions
- âœ… Deploy models

**Charts Tab:**
- âœ… Accuracy trends
- âœ… Parameter distributions
- âœ… Metric comparisons
- âœ… Performance analysis

---

## ğŸ”„ Workflow: Creating New Experiments

### Step 1: Prepare Your Code

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Import your data
# X_train, y_train, X_test, y_test = ...
```

### Step 2: Set Experiment

```python
mlflow.set_experiment("health_experiment")
```

### Step 3: Start Run

```python
with mlflow.start_run(run_name="experiment_v2"):
```

### Step 4: Log Parameters

```python
    mlflow.log_param("n_estimators", 150)
    mlflow.log_param("test_size", 0.2)
```

### Step 5: Train Model

```python
    model = RandomForestClassifier(
        n_estimators=150,
        random_state=42
    )
    model.fit(X_train, y_train)
```

### Step 6: Log Metrics

```python
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric("accuracy", accuracy)
```

### Step 7: Log Model

```python
    mlflow.sklearn.log_model(model, "model")
```

### Step 8: End Run

```python
# Automatically ends when exiting with block
```

---

## ğŸ“‹ Complete Example

```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Set experiment name
mlflow.set_experiment("health_experiment")

# Start a new run
with mlflow.start_run(run_name="hyperparameter_tuning_v1"):
    
    # ===== LOG PARAMETERS =====
    n_estimators = 150
    max_depth = 10
    random_state = 42
    test_size = 0.2
    
    mlflow.log_param("n_estimators", n_estimators)
    mlflow.log_param("max_depth", max_depth)
    mlflow.log_param("random_state", random_state)
    mlflow.log_param("test_size", test_size)
    
    # ===== TRAIN MODEL =====
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state
    )
    model.fit(X_train, y_train)
    
    # ===== GET PREDICTIONS =====
    y_pred = model.predict(X_test)
    
    # ===== CALCULATE METRICS =====
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    # ===== LOG METRICS =====
    mlflow.log_metric("accuracy", accuracy)
    mlflow.log_metric("precision", precision)
    mlflow.log_metric("recall", recall)
    mlflow.log_metric("f1_score", f1)
    
    # ===== LOG MODEL =====
    mlflow.sklearn.log_model(model, "model")
    
    # ===== LOG ARTIFACTS =====
    import json
    metrics_dict = {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1
    }
    with open("metrics.json", "w") as f:
        json.dump(metrics_dict, f)
    mlflow.log_artifact("metrics.json")
    
    print(f"Run completed: accuracy={accuracy:.4f}")
```

---

## ğŸ” Viewing Results

### In MLflow UI

```
1. Open: http://127.0.0.1:5000
2. Click on "health_experiment"
3. View "bouncy-mole-883" run
4. See metrics:
   â”œâ”€ rf_val_acc: 0.9854
   â”œâ”€ optimal_assignment_score: 102.12
   â””â”€ n_samples: 205
5. See parameters:
   â”œâ”€ n_estimators: 100
   â”œâ”€ test_size: 0.2
   â””â”€ random_state: 42
```

### Comparing Runs

```
If you have multiple runs:
1. Select 2+ runs to compare
2. Click "Compare" button
3. View side-by-side:
   â”œâ”€ All parameters
   â”œâ”€ All metrics
   â”œâ”€ Performance differences
   â””â”€ Trends
```

---

## ğŸ’¾ File Organization

### MLflow Directory Structure

```
Project Root/
â””â”€â”€ mlruns/
    â”œâ”€â”€ 0/ (Default Experiment)
    â”‚   â””â”€â”€ meta.yaml
    â”‚
    â””â”€â”€ 606211225731357525/ (health_experiment)
        â”œâ”€â”€ meta.yaml
        â””â”€â”€ 6df6f65520894bb886da66be62352ee0/ (Run)
            â”œâ”€â”€ meta.yaml
            â”œâ”€â”€ metrics/
            â”‚   â”œâ”€â”€ n_samples
            â”‚   â”œâ”€â”€ optimal_assignment_score
            â”‚   â””â”€â”€ rf_val_acc
            â”œâ”€â”€ params/
            â”‚   â”œâ”€â”€ n_estimators
            â”‚   â”œâ”€â”€ random_state
            â”‚   â””â”€â”€ test_size
            â”œâ”€â”€ tags/
            â”‚   â”œâ”€â”€ mlflow.runName
            â”‚   â”œâ”€â”€ mlflow.source.type
            â”‚   â”œâ”€â”€ mlflow.user
            â”‚   â”œâ”€â”€ mlflow.source.name
            â”‚   â””â”€â”€ mlflow.source.git.commit
            â””â”€â”€ artifacts/
                â”œâ”€â”€ model.pkl
                â””â”€â”€ ...
```

---

## ğŸ“ˆ Hyperparameter Tuning Strategy

### What to Try

```python
# Try different n_estimators
n_estimators_options = [50, 75, 100, 150, 200]

for n in n_estimators_options:
    with mlflow.start_run(run_name=f"rf_trees_{n}"):
        mlflow.log_param("n_estimators", n)
        
        model = RandomForestClassifier(n_estimators=n)
        model.fit(X_train, y_train)
        
        accuracy = model.score(X_test, y_test)
        mlflow.log_metric("accuracy", accuracy)
        
        mlflow.sklearn.log_model(model, "model")
```

### Then in MLflow UI

```
1. Open Experiments
2. Select health_experiment
3. Compare all runs
4. View metrics side-by-side
5. Select best performer
6. Register as production model
```

---

## ğŸ¯ Best Practices

### Do's âœ…

```
âœ… Log all hyperparameters
âœ… Log multiple metrics
âœ… Use meaningful run names
âœ… Track artifacts
âœ… Use fixed random seeds
âœ… Document experiment purpose
âœ… Compare runs regularly
âœ… Version your models
```

### Don'ts âŒ

```
âŒ Don't hardcode parameters
âŒ Don't skip metric logging
âŒ Don't use vague run names
âŒ Don't rely on random seeds
âŒ Don't compare without MLflow
âŒ Don't deploy untested models
âŒ Don't mix different datasets
```

---

## ğŸ“Š Metrics to Track

### Always Log

```python
# Basic metrics
mlflow.log_metric("accuracy", accuracy)
mlflow.log_metric("f1_score", f1_score)

# For classification
mlflow.log_metric("precision", precision)
mlflow.log_metric("recall", recall)
mlflow.log_metric("auc_roc", auc_roc)

# For regression
mlflow.log_metric("mse", mse)
mlflow.log_metric("rmse", rmse)
mlflow.log_metric("r2_score", r2)
```

### Feature Importance

```python
# Log feature importance
for i, importance in enumerate(model.feature_importances_):
    mlflow.log_metric(f"feature_{i}_importance", importance)
```

### Model Size

```python
import os
model_size = os.path.getsize("model.pkl")
mlflow.log_metric("model_size_bytes", model_size)
```

---

## ğŸ”— Integration with Other Tools

### With DVC (Data Version Control)
```yaml
# dvc.yaml
stages:
  train:
    cmd: python src/models/train.py
    deps:
      - src/models/train.py
      - data/processed/heart_processed.csv
    outs:
      - src/models/model.pkl
    metrics:
      - mlruns/metrics.json
```

### With GitHub
```bash
# Push MLflow runs to GitHub
git add mlruns/
git commit -m "Track experiment: rf_accuracy_98.54%"
git push origin main
```

---

## ğŸ“ Troubleshooting

### MLflow UI Won't Start

```powershell
# Check if port 5000 is in use
netstat -ano | findstr ":5000"

# Kill process on port 5000
taskkill /PID <PID> /F

# Try again
mlflow ui
```

### Can't See My Experiments

```powershell
# Check MLflow version
mlflow --version

# Verify tracking directory exists
ls mlruns/

# Check permissions
icacls mlruns /grant:r "$env:USERNAME:F"
```

### Metrics Not Appearing

```python
# Make sure you use with block
with mlflow.start_run():
    mlflow.log_metric("my_metric", 0.95)
    # Auto-ends when exiting block

# Or manually end
mlflow.start_run()
mlflow.log_metric("my_metric", 0.95)
mlflow.end_run()  # IMPORTANT!
```

---

## âœ… Verification Checklist

```
âœ… MLflow installed
âœ… Experiment created: health_experiment
âœ… Run completed: bouncy-mole-883
âœ… Metrics logged: 3 metrics
âœ… Parameters logged: 3 parameters
âœ… Artifacts saved: model + config
âœ… UI accessible: http://127.0.0.1:5000
âœ… Reproducible: random_state=42
```

---

## ğŸ‰ You're All Set!

**Next Steps:**

1. **View Dashboard:** Open http://127.0.0.1:5000
2. **Create New Runs:** Try different hyperparameters
3. **Compare Results:** Use MLflow UI comparison
4. **Select Best:** Pick top performer
5. **Register Model:** For production use

---

**Status:** ğŸŸ¢ Complete and Operational  
**Last Updated:** December 3, 2025
